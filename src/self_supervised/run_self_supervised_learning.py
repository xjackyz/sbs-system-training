import os
import argparse
import json
import logging
import time
import sys
import traceback
from datetime import datetime
from pathlib import Path
import asyncio

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.append('/home/easyai/æ¡Œé¢/sbs_system/src')

from src.self_supervised.trainer.self_supervised_manager import SelfSupervisedManager
from src.self_supervised.utils.validation_set_creator import ValidationSetCreator
from src.self_supervised.utils.progress_notifier import ProgressNotifier
from src.notification.discord_notifier import DiscordNotifier
from src.self_supervised.utils.error_handler import ErrorLogger, ErrorHandler, catch_and_log_errors

os.chdir('/home/easyai/æ¡Œé¢/sbs_system')  # è®¾ç½®å½“å‰å·¥ä½œç›®å½•

def main():
    """ä¸»å‡½æ•°"""
    log_timestamp = setup_logging()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()  # ç¡®ä¿åœ¨è¿™é‡Œè§£æå‚æ•°
    
    # åˆ›å»º SelfSupervisedManager å®ä¾‹
    manager = SelfSupervisedManager(
        csv_path=args.csv_path,
        model_path=args.model_path,
        output_dir=args.output_dir,
        window_size=args.window_size,
        stride=args.stride,
        batch_size=args.batch_size,
        effective_batch_size=args.effective_batch_size,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        validate_every=args.validate_every,
        save_every=args.save_every,
        use_lr_scheduler=args.use_lr_scheduler,
        scheduler_type=args.scheduler_type,
        scheduler_t_max=args.scheduler_t_max,
        scheduler_eta_min=args.scheduler_min_lr,
        use_early_stopping=args.use_early_stopping,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_monitor=args.early_stopping_monitor,
        checkpoint_dir=args.checkpoint_dir,
        discord_webhook=args.discord_webhook,
        save_memory=args.save_memory,
        incremental_processing=args.incremental_processing,
        incremental_batch_size=args.incremental_batch_size,
        gc_interval=args.gc_interval
    )

    

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    log_dir = os.path.join('logs', 'self_supervised')
    os.makedirs(log_dir, exist_ok=True)
    
    log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f'run_{log_timestamp}.log')
    
    # é…ç½®æ—¥å¿—è®°å½•
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    # è®¾ç½®å…¶ä»–åº“çš„æ—¥å¿—çº§åˆ«
    logging.getLogger('PIL').setLevel(logging.WARNING)
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    
    logging.info(f"æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}")
    return log_timestamp


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒè„šæœ¬')
    
    # æ•°æ®å‚æ•°ç»„
    data_group = parser.add_argument_group('æ•°æ®å‚æ•°')
    data_group.add_argument('--csv_path', type=str, required=True, help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    data_group.add_argument('--window_size', type=int, default=100, help='çª—å£å¤§å°ï¼ˆKçº¿æ•°é‡ï¼‰')
    data_group.add_argument('--stride', type=int, default=60, help='æ­¥é•¿ï¼ˆç”Ÿæˆå›¾è¡¨æ—¶çš„æ»‘åŠ¨çª—å£æ­¥é•¿ï¼‰')
    data_group.add_argument('--normalize', type=bool, default=True, help='æ˜¯å¦å½’ä¸€åŒ–æ•°æ®')
    data_group.add_argument('--output_dir', type=str, default='output', help='è¾“å‡ºç›®å½•')
    
    # éªŒè¯é›†å‚æ•°ç»„
    validation_group = parser.add_argument_group('éªŒè¯é›†å‚æ•°')
    validation_group.add_argument('--create_validation_set', action='store_true', help='æ˜¯å¦åˆ›å»ºéªŒè¯é›†')
    validation_group.add_argument('--validation_mode', type=str, default='time', choices=['time', 'random'], help='éªŒè¯é›†åˆ›å»ºæ¨¡å¼: time (åŸºäºæ—¶é—´) æˆ– random (éšæœºæŠ½æ ·)')
    validation_group.add_argument('--validation_ratio', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹ (å½“mode=randomæ—¶ä½¿ç”¨)')
    validation_group.add_argument('--validation_from_date', type=str, help='éªŒè¯é›†èµ·å§‹æ—¥æœŸ (å½“mode=timeæ—¶ä½¿ç”¨ï¼Œæ ¼å¼: YYYY-MM-DD)')
    validation_group.add_argument('--validation_to_date', type=str, help='éªŒè¯é›†ç»“æŸæ—¥æœŸ (å½“mode=timeæ—¶ä½¿ç”¨ï¼Œæ ¼å¼: YYYY-MM-DD)')
    
    # è®­ç»ƒå‚æ•°ç»„
    training_group = parser.add_argument_group('è®­ç»ƒå‚æ•°')
    training_group.add_argument('--batch_size', type=int, default=32, help='æ‰¹é‡å¤§å°')
    training_group.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    training_group.add_argument('--learning_rate', type=float, default=0.001, help='å­¦ä¹ ç‡')
    training_group.add_argument('--validate_every', type=int, default=1, help='æ¯å¤šå°‘è½®éªŒè¯ä¸€æ¬¡')
    training_group.add_argument('--accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç”¨äºå¢å¤§æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°')
    training_group.add_argument('--effective_batch_size', type=int, help='æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœæŒ‡å®šï¼Œå°†è‡ªåŠ¨è®¡ç®—æ‰€éœ€çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    lr_scheduler_group = parser.add_argument_group('å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°')
    lr_scheduler_group.add_argument('--use_lr_scheduler', action='store_true', help='æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨')
    lr_scheduler_group.add_argument('--scheduler_type', type=str, default='plateau', 
                                   choices=['plateau', 'cosine', 'step', 'cyclic'], 
                                   help='å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹')
    lr_scheduler_group.add_argument('--scheduler_patience', type=int, default=5, 
                                   help='ReduceLROnPlateauçš„è€å¿ƒå€¼ï¼Œå³ç»è¿‡å¤šå°‘ä¸ªéªŒè¯å‘¨æœŸæ€§èƒ½æœªæå‡æ‰é™ä½å­¦ä¹ ç‡')
    lr_scheduler_group.add_argument('--scheduler_factor', type=float, default=0.5, 
                                   help='ReduceLROnPlateauçš„è¡°å‡å› å­ï¼Œå­¦ä¹ ç‡å‡å°‘çš„æ¯”ä¾‹')
    lr_scheduler_group.add_argument('--scheduler_min_lr', type=float, default=1e-6, 
                                   help='æœ€å°å­¦ä¹ ç‡ï¼Œå­¦ä¹ ç‡ä¸ä¼šè¡°å‡åˆ°ä½äºæ­¤å€¼')
    lr_scheduler_group.add_argument('--scheduler_t_max', type=int, default=10, 
                                   help='CosineAnnealingLRçš„T_maxå‚æ•°ï¼ŒåŠå‘¨æœŸé•¿åº¦')
    lr_scheduler_group.add_argument('--scheduler_step_size', type=int, default=10, 
                                   help='StepLRçš„step_sizeå‚æ•°ï¼Œæ¯å¤šå°‘è½®è¡°å‡ä¸€æ¬¡')
    lr_scheduler_group.add_argument('--scheduler_gamma', type=float, default=0.1, 
                                   help='StepLRçš„gammaå‚æ•°ï¼Œè¡°å‡å› å­')
    
    # æ·»åŠ æ—©åœå‚æ•°
    early_stopping_group = parser.add_argument_group('æ—©åœå‚æ•°')
    early_stopping_group.add_argument('--use_early_stopping', action='store_true', 
                                     help='æ˜¯å¦ä½¿ç”¨æ—©åœæœºåˆ¶')
    early_stopping_group.add_argument('--early_stopping_patience', type=int, default=10, 
                                     help='æ—©åœçš„è€å¿ƒå€¼ï¼Œå³ç»è¿‡å¤šå°‘ä¸ªéªŒè¯å‘¨æœŸæ€§èƒ½æœªæå‡æ‰åœæ­¢è®­ç»ƒ')
    early_stopping_group.add_argument('--early_stopping_delta', type=float, default=0.001, 
                                     help='æ—©åœçš„æœ€å°æ”¹è¿›é‡ï¼Œä½äºæ­¤å€¼çš„æ”¹è¿›è¢«è§†ä¸ºæ²¡æœ‰æå‡')
    early_stopping_group.add_argument('--early_stopping_monitor', type=str, default='val_loss', 
                                     choices=['val_loss', 'val_accuracy'], 
                                     help='æ—©åœç›‘æ§çš„æŒ‡æ ‡')
    
    # æ¨¡å‹å‚æ•°ç»„
    model_group = parser.add_argument_group('æ¨¡å‹å‚æ•°')
    model_group.add_argument('--model_path', type=str, help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (å¦‚æœè¦ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ)')
    
    # å¥–åŠ±æœºåˆ¶å‚æ•°ç»„
    reward_group = parser.add_argument_group('å¥–åŠ±æœºåˆ¶å‚æ•°')
    reward_group.add_argument('--use_nq_contract', action='store_true', help='æ˜¯å¦ä½¿ç”¨NQåˆçº¦ä½œä¸ºå¥–åŠ±ä¿¡å·')
    reward_group.add_argument('--reward_config', type=str, help='å¥–åŠ±é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    
    # ä¿å­˜å‚æ•°ç»„
    save_group = parser.add_argument_group('ä¿å­˜å‚æ•°')
    save_group.add_argument('--save_every', type=int, default=5, help='æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹')
    save_group.add_argument('--checkpoint_dir', type=str, default='checkpoints', help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    
    # Discordé€šçŸ¥å‚æ•°ç»„
    discord_group = parser.add_argument_group('Discordé€šçŸ¥å‚æ•°')
    discord_group.add_argument('--discord_webhook', type=str, help='Discord Webhook URL')
    discord_group.add_argument('--notify_every', type=int, default=5, help='æ¯å¤šå°‘è½®é€šçŸ¥ä¸€æ¬¡è¿›åº¦')
    
    # å†…å­˜ä¼˜åŒ–å‚æ•°ç»„
    memory_group = parser.add_argument_group('å†…å­˜ä¼˜åŒ–å‚æ•°')
    memory_group.add_argument('--save_memory', action='store_true', help='å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨')
    memory_group.add_argument('--incremental_processing', action='store_true', help='å¯ç”¨å¢é‡æ•°æ®å¤„ç†ï¼Œé€‚ç”¨äºå¤§å‹æ•°æ®é›†')
    memory_group.add_argument('--incremental_batch_size', type=int, default=5000, help='å¢é‡å¤„ç†çš„æ‰¹å¤„ç†å¤§å°')
    memory_group.add_argument('--cache_process_results', action='store_true', help='å°†å¤„ç†ç»“æœç¼“å­˜åˆ°ç£ç›˜ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨')
    memory_group.add_argument('--gc_interval', type=int, default=1000, help='åƒåœ¾å›æ”¶é—´éš”(ä»¥å¤„ç†çª—å£æ•°è®¡)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--skip_chart_gen', action='store_true', help='è·³è¿‡å›¾è¡¨ç”Ÿæˆæ­¥éª¤')
    parser.add_argument('--skip_train', action='store_true', help='è·³è¿‡è®­ç»ƒæ­¥éª¤ï¼Œåªç”Ÿæˆå›¾è¡¨')
    parser.add_argument('--estimate_only', action='store_true', help='åªä¼°ç®—å¤„ç†æ—¶é—´å’Œèµ„æºä½¿ç”¨ï¼Œä¸æ‰§è¡Œå®é™…è®­ç»ƒ')
    
    # æ·»åŠ é”™è¯¯å¤„ç†å‚æ•°ç»„
    error_group = parser.add_argument_group('é”™è¯¯å¤„ç†å‚æ•°')
    error_group.add_argument('--auto_restart', action='store_true', help='è®­ç»ƒå‡ºé”™æ—¶è‡ªåŠ¨é‡å¯')
    error_group.add_argument('--max_retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    error_group.add_argument('--retry_delay', type=int, default=60, help='é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰')
    error_group.add_argument('--performance_monitoring', action='store_true', help='å¯ç”¨æ€§èƒ½ç›‘æ§')
    error_group.add_argument('--performance_threshold', type=float, default=0.05, help='æ€§èƒ½ä¸‹é™é˜ˆå€¼')
    error_group.add_argument('--monitoring_window', type=int, default=5, help='æ€§èƒ½ç›‘æ§çª—å£å¤§å°')
    
    return parser.parse_args()


def estimate_processing(args):
    """ä¼°è®¡å¤„ç†æ—¶é—´å’Œèµ„æºéœ€æ±‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        ä¼°è®¡ä¿¡æ¯å­—å…¸
    """
    # è®¡ç®—æ¯ä¸ªå›¾è¡¨çš„å†…å­˜å ç”¨
    chart_size_mb = 0.02  # å•ä¸ªå›¾è¡¨çº¦20KB
    # æ¯ä¸ªSBSåºåˆ—çš„å†…å­˜å ç”¨
    sbs_sequence_size_mb = 0.1  # çº¦100KB/åºåˆ—
    
    # åŠ è½½CSVæ–‡ä»¶å¹¶è®¡ç®—æ€»è¡Œæ•°
    with open(args.csv_path, 'r') as f:
        next(f)  # è·³è¿‡æ ‡é¢˜è¡Œ
        total_rows = sum(1 for _ in f)
    
    logging.info(f"æ•°æ®æ–‡ä»¶: {args.csv_path}, æ€»è¡Œæ•°: {total_rows}")
    
    # è®¡ç®—å›¾è¡¨æ•°é‡
    total_charts = (total_rows - args.window_size) // args.stride + 1
    if total_charts <= 0:
        total_charts = 1
    
    logging.info(f"çª—å£å¤§å°: {args.window_size}, æ­¥é•¿: {args.stride}")
    logging.info(f"ä¼°è®¡å›¾è¡¨æ•°é‡: {total_charts}")
    
    # ä¼°è®¡å¤„ç†æ—¶é—´
    chart_processing_time = 0.15  # æ¯ä¸ªå›¾è¡¨å¹³å‡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
    total_processing_time = total_charts * chart_processing_time  # è®¡ç®—æ€»å¤„ç†æ—¶é—´
    total_hours = total_processing_time / 3600
    
    logging.info(f"å¹³å‡æ¯å¼ å›¾è¡¨å¤„ç†æ—¶é—´: {chart_processing_time:.4f} ç§’")
    logging.info(f"ä¼°è®¡æ€»å¤„ç†æ—¶é—´: {total_hours:.2f} å°æ—¶ ({total_processing_time:.2f} ç§’)")
    
    # ä¼°è®¡å†…å­˜éœ€æ±‚
    estimated_memory_mb = total_charts * chart_size_mb
    estimated_sbs_memory_mb = total_charts * 0.05 * sbs_sequence_size_mb  # å‡è®¾5%çš„å›¾è¡¨å½¢æˆSBSåºåˆ—
    total_memory_mb = estimated_memory_mb + estimated_sbs_memory_mb
    
    # è€ƒè™‘å†…å­˜ä¼˜åŒ–
    if args.save_memory or args.incremental_processing:
        # å¢é‡å¤„ç†æ¨¡å¼ä¸‹å†…å­˜å ç”¨é™ä½
        memory_reduction_factor = 0.3 if args.incremental_processing else 0.7
        total_memory_mb = total_memory_mb * memory_reduction_factor
        logging.info(f"å¯ç”¨å†…å­˜ä¼˜åŒ–ï¼Œé¢„è®¡å†…å­˜éœ€æ±‚å‡å°‘è‡³åŸå§‹éœ€æ±‚çš„ {int(memory_reduction_factor*100)}%")
    
    logging.info(f"ä¼°è®¡å†…å­˜éœ€æ±‚: {total_memory_mb:.2f} MB ({total_memory_mb/1024:.2f} GB)")
    
    # è¿”å›ä¼°è®¡ä¿¡æ¯
    return {
        'total_rows': total_rows,
        'total_charts': total_charts,
        'processing_time_seconds': total_processing_time,
        'processing_time_hours': total_hours,
        'memory_mb': total_memory_mb
    }


def setup_progress_notifier(args, log_timestamp):
    """è®¾ç½®è¿›åº¦é€šçŸ¥å™¨
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        log_timestamp: æ—¥å¿—æ—¶é—´æˆ³
        
    Returns:
        è¿›åº¦é€šçŸ¥å™¨å®ä¾‹æˆ–None
    """
    if not args.discord_webhook:
        return None
    
    # åˆ›å»ºè¿›åº¦é€šçŸ¥å™¨
    notifier = ProgressNotifier(
        interval_minutes=args.notify_every
    )
    
    # å‘é€å¯åŠ¨é€šçŸ¥
    try:
        config_summary = {
            'csv_file': os.path.basename(args.csv_path),
            'window_size': args.window_size,
            'stride': args.stride,
            'batch_size': args.batch_size,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'log_file': f'logs/self_supervised/run_{log_timestamp}.log',
            'estimated_time': f'é¢„è®¡è®­ç»ƒæ—¶é—´: {args.epochs * (total_processing_time / args.batch_size):.2f}å°æ—¶'
        }
        notifier.send_training_started(config_summary)
        logger.info('å·²å‘é€å¯åŠ¨é€šçŸ¥')

        # è®¾ç½®é˜¶æ®µæ€§æ¨é€
        asyncio.create_task(notifier.on_periodic_notification())
    except Exception as e:
        logger.error(f'å‘é€å¯åŠ¨é€šçŸ¥æ—¶å‡ºé”™: {e}')
    
    return notifier


def create_validation_set(args):
    """åˆ›å»ºéªŒè¯é›†
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        éªŒè¯é›†ä¿¡æ¯
    """
    if not args.create_validation_set:
        logging.info("æœªå¯ç”¨éªŒè¯é›†åˆ›å»º")
        return None
    
    logging.info("å¼€å§‹åˆ›å»ºéªŒè¯é›†...")
    
    # åˆ›å»ºéªŒè¯é›†åˆ›å»ºå™¨
    validation_creator = ValidationSetCreator(
        base_data_path=args.csv_path,
        output_dir="data/validation"
    )
    
    validation_info = None
    
    # æ ¹æ®éªŒè¯é›†ç±»å‹åˆ›å»ºéªŒè¯é›†
    if args.validation_mode == 'time':
        validation_info = validation_creator.create_date_range_validation(
            start_date=args.validation_from_date,
            end_date=args.validation_to_date
        )
        logging.info(f"å·²åˆ›å»ºæ—¥æœŸèŒƒå›´éªŒè¯é›†: {args.validation_from_date} åˆ° {args.validation_to_date}")
    elif args.validation_mode == 'random':
        validation_info = validation_creator.create_random_validation(
            ratio=args.validation_ratio
        )
        logging.info(f"å·²åˆ›å»ºéšæœºéªŒè¯é›†ï¼Œæ¯”ä¾‹: {args.validation_ratio}")
    
    if validation_info:
        logging.info(f"éªŒè¯é›†åˆ›å»ºå®Œæˆ: {validation_info.get('name')}")
        logging.info(f"åŒ…å« {validation_info.get('data_points')} ä¸ªæ•°æ®ç‚¹")
    else:
        logging.warning("éªŒè¯é›†åˆ›å»ºå¤±è´¥æˆ–è¿”å›ç©ºä¿¡æ¯")
    
    return validation_info


def main():
    """ä¸»å‡½æ•°"""
    log_timestamp = setup_logging()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    log_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # å¦‚æœåªæ˜¯ä¼°ç®—ï¼Œåˆ™è°ƒç”¨ä¼°ç®—å‡½æ•°å¹¶é€€å‡º
    if args.estimate_only:
        estimate_processing(args)
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    logs_dir = os.path.join(args.output_dir, 'logs')
    os.makedirs(logs_dir, exist_ok=True)
    
    # è®¡ç®—æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°å’Œç´¯ç§¯æ­¥æ•°
    if args.effective_batch_size:
        if args.effective_batch_size < args.batch_size:
            logger.warning(f"æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°({args.effective_batch_size})å°äºå®é™…æ‰¹å¤„ç†å¤§å°({args.batch_size})ï¼Œå°†ä½¿ç”¨å®é™…æ‰¹å¤„ç†å¤§å°")
            args.accumulation_steps = 1
        else:
            # è®¡ç®—æ‰€éœ€çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            args.accumulation_steps = max(1, args.effective_batch_size // args.batch_size)
            logger.info(f"æœ‰æ•ˆæ‰¹å¤„ç†å¤§å°: {args.effective_batch_size}, å®é™…æ‰¹å¤„ç†å¤§å°: {args.batch_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {args.accumulation_steps}")
    
    # åˆå§‹åŒ–Discordé€šçŸ¥å™¨
    discord_notifier = None
    if args.discord_webhook:
        try:
            discord_notifier = DiscordNotifier()
            logger.info("å·²åˆå§‹åŒ–Discordé€šçŸ¥å™¨")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–Discordé€šçŸ¥å™¨å¤±è´¥: {e}")
    
    # åˆå§‹åŒ–é”™è¯¯æ—¥å¿—è®°å½•å™¨å’Œé”™è¯¯å¤„ç†å™¨
    error_logger = ErrorLogger(
        log_dir=logs_dir,
        notifier=discord_notifier,
        save_system_info=True
    )
    
    error_handler = ErrorHandler(
        error_logger=error_logger,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay,
        auto_restart=args.auto_restart
    )
    
    # è¯»å–å¥–åŠ±é…ç½®
    reward_config = None
    if args.reward_config:
        try:
            with open(args.reward_config, 'r') as f:
                reward_config = json.load(f)
        except Exception as e:
            error_logger.log_error(e, {'file': args.reward_config}, 
                                 recovery_suggestion="è¯·æ£€æŸ¥å¥–åŠ±é…ç½®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ–‡ä»¶æ ¼å¼ä¸ºæœ‰æ•ˆçš„JSONã€‚")
            logger.error(f"æ— æ³•åŠ è½½å¥–åŠ±é…ç½®: {e}")
            reward_config = None
    
    # åˆ›å»ºè¿›åº¦é€šçŸ¥å™¨
    progress_notifier = ProgressNotifier(
        discord_notifier=discord_notifier,
        notify_every=args.notify_every,
        charts_dir=os.path.join(args.output_dir, 'charts')
    )
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å‘é€å¼€å§‹è®­ç»ƒé€šçŸ¥
    if discord_notifier:
        training_config = {
            'batch_size': args.batch_size,
            'effective_batch_size': args.effective_batch_size or args.batch_size,
            'epochs': args.epochs,
            'learning_rate': args.learning_rate,
            'csv_path': os.path.basename(args.csv_path),
            'window_size': args.window_size,
            'stride': args.stride,
            'memory_optimization': args.save_memory,
            'incremental_processing': args.incremental_processing
        }
        
        progress_notifier.send_training_started(training_config)
    
    try:
        # åˆ›å»ºSelfSupervisedManagerå®ä¾‹
        manager = SelfSupervisedManager(
            csv_path=args.csv_path,
            model_path=args.model_path,
            output_dir=args.output_dir,
            window_size=args.window_size,
            stride=args.stride,
            batch_size=args.batch_size,
            effective_batch_size=args.effective_batch_size,
            epochs=args.epochs,
            learning_rate=args.learning_rate,
            validate_every=args.validate_every,
            save_every=args.save_every,
            use_lr_scheduler=args.use_lr_scheduler,
            scheduler_type=args.scheduler_type,
            scheduler_t_max=args.scheduler_t_max,
            scheduler_eta_min=args.scheduler_min_lr,
            use_early_stopping=args.use_early_stopping,
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_monitor=args.early_stopping_monitor,
            checkpoint_dir=args.checkpoint_dir,
            discord_webhook=args.discord_webhook,
            save_memory=args.save_memory,
            incremental_processing=args.incremental_processing,
            incremental_batch_size=args.incremental_batch_size,
            gc_interval=args.gc_interval
        )
        
        # è®¾ç½®å†…å­˜ä¼˜åŒ–é€‰é¡¹
        if args.save_memory or args.incremental_processing:
            manager.save_memory = args.save_memory
            manager.use_incremental_processing = args.incremental_processing
            manager.incremental_batch_size = args.incremental_batch_size
            logger.info(f"å¯ç”¨å†…å­˜ä¼˜åŒ–: {'æ˜¯' if args.save_memory else 'å¦'}, å¢é‡å¤„ç†: {'æ˜¯' if args.incremental_processing else 'å¦'}")
        
        # è®¾ç½®éªŒè¯é›†åˆ›å»ºé€‰é¡¹
        if args.create_validation_set:
            validation_params = {
                'mode': args.validation_mode,
                'ratio': args.validation_ratio,
            }
            
            if args.validation_mode == 'time' and args.validation_from_date and args.validation_to_date:
                validation_params['from_date'] = args.validation_from_date
                validation_params['to_date'] = args.validation_to_date
            
            manager.set_validation_params(**validation_params)
        
        # è®¾ç½®æ€»è½®æ¬¡ä»¥ä¾¿è¿›åº¦é€šçŸ¥
        if progress_notifier:
            progress_notifier.set_total_epochs(args.epochs)
        
        # è¿è¡Œè‡ªç›‘ç£å­¦ä¹ 
        manager.run_self_supervised_learning(
            skip_chart_gen=args.skip_chart_gen,
            save_every=args.save_every,
            validate_every=args.validate_every,
            checkpoint_dir=args.checkpoint_dir,
            accumulation_steps=args.accumulation_steps
        )
        
        # å¦‚æœä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œè®¾ç½®è°ƒåº¦å™¨
        if args.use_lr_scheduler:
            scheduler_params = {
                'factor': args.scheduler_factor,
                'patience': args.scheduler_patience,
                'min_lr': args.scheduler_min_lr,
                'T_max': args.scheduler_t_max,
                'step_size': args.scheduler_step_size,
                'gamma': args.scheduler_gamma
            }
            trainer = manager.get_trainer()
            if trainer:
                trainer.setup_lr_scheduler(
                    scheduler_type=args.scheduler_type,
                    **scheduler_params
                )
                logger.info(f"è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨: {args.scheduler_type}")
        
        # å¦‚æœä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œè®¾ç½®æ—©åœ
        if args.use_early_stopping:
            trainer = manager.get_trainer()
            if trainer:
                trainer.setup_early_stopping(
                    patience=args.early_stopping_patience,
                    min_delta=args.early_stopping_delta,
                    monitor=args.early_stopping_monitor
                )
                logger.info(f"è®¾ç½®æ—©åœæœºåˆ¶: {args.early_stopping_monitor}, è€å¿ƒåº¦: {args.early_stopping_patience}")
        
        # è®¾ç½®æ€§èƒ½ç›‘æ§
        if args.performance_monitoring:
            if trainer:
                trainer.setup_performance_monitoring(
                    threshold=args.performance_threshold,
                    window_size=args.monitoring_window
                )
                logger.info(f"è®¾ç½®æ€§èƒ½ç›‘æ§: é˜ˆå€¼={args.performance_threshold}, çª—å£å¤§å°={args.monitoring_window}")
        
        # è®¡ç®—æ€»è®­ç»ƒæ—¶é—´
        total_time = time.time() - start_time
        hours, remainder = divmod(int(total_time), 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds}ç§’" if hours > 0 else f"{minutes}åˆ†é’Ÿ{seconds}ç§’"
        
        logger.info(f"è‡ªç›‘ç£å­¦ä¹ å®Œæˆ! æ€»è€—æ—¶: {time_str}")
        
        # åˆ›å»ºå¹¶å‘é€è®­ç»ƒæ€»ç»“
        if discord_notifier:
            trainer = manager.get_trainer()
            if trainer:
                # è·å–æœ€ç»ˆæŒ‡æ ‡
                final_metrics = trainer.get_final_metrics() if hasattr(trainer, 'get_final_metrics') else {}
                best_metrics = trainer.get_best_metrics() if hasattr(trainer, 'get_best_metrics') else {}
                
                # åˆ›å»ºæ€»ç»“å›¾è¡¨
                summary_chart = progress_notifier.create_training_summary_chart()
                charts = [summary_chart] if summary_chart else []
                
                # å‘é€å®Œæˆé€šçŸ¥
                discord_notifier.send_message(
                    f"ğŸ‰ **è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå®Œæˆ!** ğŸ‰\n\n"
                    f"**æ€»è®­ç»ƒæ—¶é—´:** {time_str}\n"
                    f"**æ€»è½®æ¬¡:** {args.epochs}\n"
                    f"**CSVæ•°æ®:** {os.path.basename(args.csv_path)}\n"
                    f"**çª—å£å¤§å°:** {args.window_size}\n"
                    f"**æ­¥é•¿:** {args.stride}\n\n"
                    f"**æœ€ç»ˆæŒ‡æ ‡:**\n" + 
                    "\n".join([f"- {k}: {v:.4f}" for k, v in final_metrics.items()]) +
                    (f"\n\n**æœ€ä½³æŒ‡æ ‡:**\n" + "\n".join([f"- {k}: {v:.4f}" for k, v in best_metrics.items()]) if best_metrics else ""),
                    files=charts
                )
    
    except Exception as e:
        # è·å–å¼‚å¸¸ä¿¡æ¯
        exc_type, exc_value, exc_traceback = sys.exc_info()
        tb_str = ''.join(traceback.format_exception(exc_type, exc_value, exc_traceback))
        
        # è®°å½•é”™è¯¯
        logger.exception("è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        
        # è®°å½•åˆ°é”™è¯¯æ—¥å¿—
        error_context = {
            'phase': 'main',
            'args': {k: v for k, v in vars(args).items() if not k.startswith('_')}
        }
        error_logger.log_error(e, error_context, notify=True)
        
        # å‘é€é”™è¯¯é€šçŸ¥
        if discord_notifier:
            try:
                discord_notifier.send_message(
                    f"âŒ **è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯** âŒ\n\n"
                    f"**é”™è¯¯ç±»å‹:** {type(e).__name__}\n"
                    f"**é”™è¯¯æ¶ˆæ¯:** {str(e)}\n\n"
                    f"è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ã€‚\n\n"
                    f"**å‘ç”Ÿæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                )
            except Exception as notify_error:
                logger.error(f"å‘é€é”™è¯¯é€šçŸ¥æ—¶å‡ºé”™: {notify_error}")
        
        # æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿è¿›ç¨‹ä»¥éé›¶çŠ¶æ€é€€å‡º
        raise


if __name__ == "__main__":
        main()